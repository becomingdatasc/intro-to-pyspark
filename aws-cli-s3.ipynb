{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon S3\n",
    "\n",
    "### What it is S3\n",
    "\n",
    "__Amazon S3__ (Simple Storage Service) is a Amazon's service for storing files. It is simple in a sense that one store data using the follwing:\n",
    "* __bucket__: place to store. Its name is unique for all S3 users, which means that there cannot exist two buckets with the same name even if they are private for to different users.\n",
    "* __key__: a unique (for a bucket) name that link to the sotred object. It is common to use path like syntax to group objects. \n",
    "* __object__: any file (text or binary). It can be partitioned.\n",
    "\n",
    "### Sign up\n",
    "First go to \n",
    "<https://s3.console.aws.amazon.com/s3>\n",
    "\n",
    "and sign up for S3. You can also try to create a bucket, upload files etc. Here we will explain how to use it porogramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "But first let's get data we are going to use here. We take the dataset `train.csv` from <https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge>. \n",
    "We locally store in `data` directory.\n",
    "\n",
    "### Sampling data\n",
    "\n",
    "We also sample this dataset in order to have one more example (and faster execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(10)\n",
    "comments = pd.read_csv(\"data/train.csv\")\n",
    "nrows = comments.shape[0]\n",
    "comments.iloc[np.random.choice(range(nrows), 10000, replace=False)]\\\n",
    "    .to_csv(\"data/train_sample10000.csv\", index=False)\n",
    "comments.iloc[np.random.choice(range(nrows), 1000, replace=False)]\\\n",
    "    .to_csv(\"data/train_sample1000.csv\", index=False)\n",
    "comments.iloc[np.random.choice(range(nrows), 100, replace=False)]\\\n",
    "    .to_csv(\"data/train_sample100.csv\", index=False)\n",
    "comments10 = comments.iloc[np.random.choice(range(nrows), 10, replace=False)]\n",
    "comments10.to_csv(\"data/train_sample10.csv\", index=False)\n",
    "comments10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing AWS Command Line Interface and boto\n",
    "\n",
    "In order to install boto (Python interface to Amazon Web Service) and AWS Command Line Interface (__CLI__) type:\n",
    "```\n",
    "pip install boto3\n",
    "pip install awscli\n",
    "```\n",
    "\n",
    "Then in your home directory create file `~/.aws/credentials` with the following:\n",
    "\n",
    "```\n",
    "[myaws]\n",
    "aws_access_key_id = YOUR_ACCESS_KEY\n",
    "aws_secret_access_key = YOUR_SECRET_KEY\n",
    "```\n",
    "\n",
    "If you add these configuration as `[default]`, you won't need to add `--profile myaws` in CLI commands in Section CLI Basic Commands.\n",
    "\n",
    "### Where to get credentials from\n",
    "\n",
    "1. Go to https://console.aws.amazon.com/console/home and log in\n",
    "2. Click on USER NAME (right top) and select `My Security Credentials`.\n",
    "3. Click on `+ Access keys (access key ID and secret access key)` and then on `Create New Acess Key`.\n",
    "4 Choose `Show access key`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Basic Commands \n",
    "\n",
    "### List buckets\n",
    "```\n",
    "aws --profile myaws s3 ls\n",
    "```\n",
    "\n",
    "### List all buckets\n",
    "\n",
    "```\n",
    "aws --profile myaws s3 ls \n",
    "```\n",
    "\n",
    "### Create buckers\n",
    "```\n",
    "aws --profile myaws s3 mb s3://barteks-toxic-comments\n",
    "```\n",
    "__Warning__ The bucket namespace is shared by all users of the system so you need to change the name.\n",
    "\n",
    "### Upload and download files\n",
    "\n",
    "#### Upload\n",
    "```\n",
    "aws --profile myaws s3 cp data/train.csv s3://barteks-toxic-comments\n",
    "aws --profile myaws s3 cp data/train_sample10000.csv s3://barteks-toxic-comments/sample/\n",
    "aws --profile myaws s3 cp data/train_sample1000.csv s3://barteks-toxic-comments/sample/\n",
    "aws --profile myaws s3 cp data/train_sample100.csv s3://barteks-toxic-comments/sample/\n",
    "aws --profile myaws s3 cp data/train_sample10.csv s3://barteks-toxic-comments/sample/\n",
    "```\n",
    "\n",
    "The last 4 commands can be done in shell calling:\n",
    "```\n",
    "for f in data/train_sample1*.csv; do aws --profile myaws s3 cp $f s3://barteks-toxic-comments/sample/; done\n",
    "```\n",
    "\n",
    "#### Download\n",
    "```\n",
    "aws --profile myaws s3 cp s3://barteks-toxic-comments/sample/train_sample10.csv data/train_copy_sample10.csv\n",
    "```\n",
    "\n",
    "### List files in path\n",
    " \n",
    "```\n",
    "aws --profile myaws s3 ls s3://barteks-toxic-comments/\n",
    "aws --profile myaws s3 ls s3://barteks-toxic-comments/sample/\n",
    "```\n",
    "\n",
    "### Remove file(s)\n",
    "\n",
    "```\n",
    "aws --profile myaws s3 rm s3://barteks-toxic-comments/sample/train_sample2.csv\n",
    "aws --profile myaws s3 rm s3://barteks-toxic-comments/sample/ --recursive\n",
    "```\n",
    "\n",
    "### Delete bucket\n",
    "\n",
    "For deleting a bucket use\n",
    "```\n",
    "aws --profile myaws s3 rb  s3://barteks-toxic-comments\n",
    "```\n",
    "in order to delete non empty backet use `--force` option.\n",
    "\n",
    "In order to empty a backet use\n",
    "```\n",
    "aws --profile myaws s3 rm s3://barteks-toxic-comments/ --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Boto is\n",
    "\n",
    "Boto is a Python package that provides interfaces to Amazon Web Services. Here we are focused on its application to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating S3 Resource\n",
    "\n",
    "We start using boto3 by creating S3 resorce object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "session = boto3.Session(profile_name='myaws')\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From evironment variables\n",
    "\n",
    "If your credentials are stored as evirionment variables `AWS_SECRET_KEY_ID` and `AWS_SECRET_ACCESS_KEY` then you can do the following:\n",
    "\n",
    "```\n",
    "import os\n",
    "aws_access_key_id = os.environ.get('AWS_SECRET_KEY_ID')\n",
    "aws_secret_access_key = s.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id, \n",
    "    aws_secret_access_key=aws_secret_access_key)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(s3.buckets.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bucket\n",
    "\n",
    "__Warning__ As before, bucket's namespace is shared, so the following command may not poroduce a bucket if a bucket with the name exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3.create_bucket(\n",
    "#    ACL='public-read',\n",
    "#    Bucket=\"barteks-toxic-comments-stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you have the followng Access Control List (ACL) options while creating it: \n",
    "* `'private', \n",
    "* 'public-read', \n",
    "* 'public-read-write', \n",
    "* 'authenticated-read'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket = s3.Bucket('barteks-toxic-comments-stats')\n",
    "#bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List keys in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('barteks-toxic-comments')\n",
    "objs = [obj for obj in bucket.objects.all()]\n",
    "objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[obj.key for obj in bucket.objects.filter(Prefix=\"sample/\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of class `ObjectSummary` has to properties `Bucket` (that returns Bucket object), `bucket_name` and `key` that return strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs[0].Bucket(), objs[0].bucket_name, objs[0].key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter keys and sort them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [obj for obj in bucket.objects.filter(Prefix=\"sample/\")]\n",
    "objects.sort(key=lambda obj: obj.key, reverse=True)\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket('barteks-toxic-comments')\n",
    "bucket.download_file('sample/train_sample10.csv', \"data/train_copy2_sample10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform to pandas.DataFrame\n",
    "\n",
    "One way to do this is to download the file and open it with `pandas.read_csv` method. If we do not want to do this we have to read it a buffer and open it from there. In order to do this we need to use low level interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "obj = s3.Object('barteks-toxic-comments', 'sample/train_sample100.csv').get()\n",
    "comments100 = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "comments100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way, using higher level `download_fileobj` requires transform bytes streaiming into text streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "bucket.download_fileobj('sample/train_sample10.csv', f)\n",
    "f.seek(0)\n",
    "pd.read_csv(io.TextIOWrapper(f, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_bucket = s3.Bucket(\"barteks-toxic-comments-stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments100stat = \\\n",
    "    comments100.groupby([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\\\n",
    "    .count().reset_index()\n",
    "comments100stat.to_csv(\"data/train_sample100stat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_bucket.upload_file(\"data/train_sample100stat.csv\", 'sample/train_sample100stat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bucket.objects.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "f = io.StringIO()\n",
    "comments100stat.to_csv(f, index=False)\n",
    "stat_bucket.upload_fileobj(f, 'sample/train_sample100stat_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bucket.objects.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3.Object('barteks-toxic-comments', 'sample/train_copy2_sample10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 client: low level access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access through http(s)\n",
    "\n",
    "### Change Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3.Object('barteks-toxic-comments-stats', 'sample/train_sample100stat_copy.csv')\n",
    "obj.Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uri\n",
    "\n",
    "There are two formats of uri:\n",
    "```\n",
    "http(s)://s3.amazonaws.com/<bucket>/<object>\n",
    "http(s)://<bucket>.s3.amazonaws.com/<object>\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "<https://s3.amazonaws.com/barteks-toxic-comments-stats/sample/train_sample100stat_copy.csv>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with smart_open\n",
    "\n",
    "### Install\n",
    "\n",
    "```\n",
    "pip install smart_open\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "\n",
    "comments1000 = \\\n",
    "    pd.read_csv(\n",
    "        smart_open(\n",
    "            's3://barteks-toxic-comments/sample/train_sample1000.csv', 'rb', \n",
    "            profile_name='myaws'))\n",
    "    \n",
    "comments1000_stat =\\\n",
    "    comments1000.groupby([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\\\n",
    "    .count().reset_index()\n",
    "comments1000_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(smart_open(\n",
    "    's3://barteks-toxic-comments/sample/train_sample100.csv', 'rb', \n",
    "        s3_session=session)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is smart enough to recognize from where it has to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(smart_open(\n",
    "    'data/train_sample100.csv', 'rb', \n",
    "    s3_session=session)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with smart_open('s3://barteks-toxic-comments-stats/sample/train_sample1000stat123.csv', 'w', \n",
    "               profile_name='myaws') as fout:\n",
    "    comments1000_stat.to_csv(fout, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.attr = 123\n",
    "        \n",
    "model = Model()\n",
    "\n",
    "with smart_open(\"s3://barteks-toxic-comments-stats/models/model.pickle\", 'wb', \n",
    "               profile_name='myaws') as f:\n",
    "    pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(stat_bucket.objects.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with smart_open(\"s3://barteks-toxic-comments-stats/models/model.pickle\", 'rb', \n",
    "               profile_name='myaws') as f:\n",
    "    model = pickle.load(f)\n",
    "print(model.attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links:\n",
    "\n",
    "* https://github.com/boto/boto3\n",
    "* https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "* https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
